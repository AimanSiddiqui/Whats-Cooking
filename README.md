# Whats-Cooking

With the growth of recipe sharing services, online cooking recipes associated with ingredients and cooking procedures are available. 
Many recipe sharing sites have given to the development of recipe recommendation mechanism. 
While most food related research has been on recipe recommendation, little effort has been done on analyzing the correlation between recipe cuisines and ingredients. 
This is a relatively new area, with few systems that focus on analyzing user given ingredients to predict the type of a cuisine (e.g. “Chinese” or “Mexican”). 
We aim to investigate the underlying cuisine-ingredient connections by exploiting the classification techniques, including associative classification and support vector machine.
Our study conducted on Kaggle data provides insights about which cuisines are the most similar and what are the essential ingredients for a cuisine, with an application to automatic cuisine labeling for recipes. 
Variations of popular approaches used in other areas, such as Rocchio’s algorithm for document classification, can be adapted to predict cuisine for given ingredients.

## Dataset and Features:

The data we are using is from the Keggle “What’s Cooking?” competition. There are training set and testing set in the data and both in JSON format. The JSON file format is understandable in this case as the data is mainly textual data and that is non-standard format of data. There are total two JSON files test and train data. Training data set contain total 39774 recipes along three variables and Test data set contain total 9944 recipes and only two variables. one example of recipe record in train. json:
{"id": 24717,
"cuisine": "indian”,
ingredients": ["turmeric", "vegetable stock", "tomatoes", "garam, masala", "naan", "red lentils", "red chili peppers", "onions", "spinach", "sweet potatoes"]}
Training data set having information about recipes inside recipe we have three main fields cuisine, ingredients and ID. Cuisines – This imply the type of cuisine that the particular recipe belongs too. There are a total 20 cuisine types and it’s in important to know these are the groups and class into which our prediction can fall into. ID - that is used to uniquely identified each and every recipe. Ingredients – Ingredients are used to make this particular recipe. Every single recipe has a different number of ingredients and different ingredients in each recipe. There are total 6000 kinds of ingredients in the all recipes. The number of ingredients per recipe is 9 in average. Test data set contain 9944 recipes, ID and ingredients.ID and ingredients have same characteristics as the id column in training data. We have to determine which cuisine it belongs too.



# Method
## Data Exploration
Before training any machine learning algorithms, we first examined the data to get a feel for its general structure. The training file contains 39,774 entries described in 666,921 lines. These are fairly modest numbers that do not require sophisticated parallelism or out-of-core algorithms. A cursory glance at 5 random entries in the training showed that the structure that the ingredients list does not contain stop words (i.e. words with relatively little semantic meaning, like “the”). However, some of the ingredients have accents. We decided to normalize these words by turning them into their unaccented counterparts to prevent issues where one version of the word is
accented and another is not. On the basis of this initial data exploration, we decided to use a fairly standard approach in Python.

## Transforming Input Data
The approach we used for representing the textual data is “Working with Text Data”.
We wrote a basic custom parser to remove accents from the lists of ingredients and concatenate each list into one string, or document. This method removes a small amount of structure from the text, since we can no longer represent the list boundaries between ingredients. Nevertheless, this is a relatively small price to pay, as we are not giving up clause, sentence, or paragraph structure (which is typical in other text classification tasks). The most basic and popular method of representing text for machine learning is the bag-of-words approach. Using this approach, we represent lists of ingredients by vectors of counts of each of the different words in the vocabulary. This is quite feasible given the relatively small size of our data set: the resulting vocabulary contained∼4000 words. We further improved on these vectors with two optimizations built into scikit-learn. First, we divided each of the word counts by the total number of words in that document to get term frequencies. This helps us normalize over various document lengths. The second optimization we performed was to weight words based on the inverse of number of documents they occur in. This helps us remove focus from words that are common to many documents and are therefore not very informative for distinguishing cuisines. To evaluate each algorithm’s ability to generalize, we divided the training data into two vectors: one containing 35000 points on which to train the algorithms, and another of the remaining 4774 points for evaluation. In the remainder of this report, “empirical accuracy” refers to the percentage of the 35000 training points that are correctly labeled, while “generalization accuracy” refers to the percentage of the other 4774 points that are correctly labeled.


## Rocchio’s Algorithm:
One popular extension of the vector space model for information retrieval relates to the usage of relevance feedback. Rocchio’s algorithm is a widely used relevance feedback method that operates in the vector space model. It allows users to rate documents returned by a retrieval system according to their information needs, later averaging this information to improve the retrieval. Rocchio’s method can also be used as a classifier for content-based filtering. Documents are represented as vectors, where each component corresponds to a term, usually a word. The weight attributed to each word can be computed using the TF-IDF scheme. Using relevance feedback, document vectors of positive and negative examples are combined into a prototype vector for each class c. These prototype vectors represent the learning process in this algorithm. New documents are then classified according to the similarity between the prototype vector of each class and the corresponding document vector, using for example the well-known
cosine similarity metric. The document is then assigned to the class whose document vector has the highest similarity value.
More specifically, Rocchio’s method computes a prototype vector →ci = (w1i, …, w|T|i) for each class ci, being T the vocabulary composed by the set of distinct terms in the training set. The weight for each term is given by the following formula:

https://wikimedia.org/api/rest_v1/media/math/render/svg/61d961eee905b400a73875d212a84ee76e644f79

In the formula, POSi and NEGi represent the positive and negative examples in the training set for class cj, and wkj is the TF-IDF weight for term k in document dj. Parameters β and γ control the influence of the positive and negative examples. The document dj is assigned to the class ci with the highest similarity value between the prototype vector →ci and the document vector →dj.




## Results/Experiments:
The idea of considering ingredients in a recipe as similar to words in a document lead to the variation of TF-IDF weights developed. This work presented good results in retrieving the user’s favorite ingredients. As previously mentioned, the TF-IDF scheme can be used to attribute weights to words when using the popular Rocchio algorithm. Instead of simply obtaining the users’ favorite ingredients using the TF-IDF variation, the user’s overall preference in ingredients could be estimated through the prototype vector, which represents the learning in Rocchio’s algorithm. These vectors would contain the user’s preferences, where the positive and negative examples are obtained directly from the user’s rated recipes/dishes.
Datasets that contain user reviews on recipes or restaurant meals are presently very hard to find. Epicurious and Food.com, which will be presented next, are food related datasets with relevant information on the recipes that contain rating events from users to recipes. In order to validate the methods explored in this work, the recommendation system also needs to return a rating value. Rocchio’s algorithm returns a similarity value between the recipe features vector and the user profile vector, so a method is needed to translate the similarity into a rating. Next, two methods are presented to translate the similarity value into a rating. There are many types of normalization methods available, the technique chosen for this work was Min-Max Normalization.

## Conclusion
The Rocchio algorithm was never explored in cuisine prediction, so various approaches were tested to build the users’ prototype vector and transform the similarity value, returned by the algorithm, into a rating value needed to compute the performance of the prediction system. After determining the best approach to adapt the Rocchio algorithm to cuisine prediction, the similarity threshold test was performed to adjust the algorithm and seek improvements in the prediction results. The final results of the experimental component showed improvements in the prediction performance using the keggle dataset.
The problem of finding cuisine-based ingredients is very much like topic modeling. We could try various techniques that we use in topic modeling like LDA to model cuisines. Also, instead of using individual ingredients as features we could try an n-gram model. This would increase the number of features but we can use PCA to get a lower number of features. There could be a low dimensional structure in cuisines with respect to ingredients. We could do SVD on these to find the low dimensional structure.
